<br> 
<center><img src="http://i.imgur.com/tveTlt8.png" width="300"></center>

## Curso: Estadística Para el Análisis Político II<br> Semestre 2017-II<br> 
### Prof. José Manuel Magallanes, PhD 
____
## **Análisis Multivariado**
## Técnicas Descriptivas y Exploratorias Multivariadas (Parte I)
____

<a id='beginning'></a>


En este curso trabajaremos técnicas multivariadas. Lo complejo es justamente la cantidad de variables que intervienen y que requieren muchas veces mucho tratamiento. La mayoría de programas estadísticos (SPSS, STATA, R, etc.) tiene diversas funciones que ayudan a alistar los datos necesarios cuando se tienen muchas variables involucradas. **Esto no era así cuando sólo se trabajaban con dos variables**. **R** es muy versatil para preparar los datos para el análisis multivariado. Usted puede utilizar otro software, como **EXCEL**, para alistarlos, pero si la cantidad de datos es numerosa, pronto encontrará que son muy limitados. 

## AGRUPANDO LAS UNIDADES DE ESTUDIO

En general, las tablas de datos se estructuran como una colección de filas (casos o unidades de estudio) y columnas (variables). En este caso, nos interesa agrupar las unidades de estudio (filas). Para este caso, utilizaremos la data creada por el *PNUD*, el Indice de Densidad del Estado. Este índice es producto de combinar otras variables. 

La data original del IDE-PNUD está en Excel, lo cual no sería mayor problema, sino fuera por que las _hojas_ tienen varios elementos innecesarios para nuestro análisis. Esta data necesita ser tratada para dejarla lista para el análisis multivariado. Este proceso está descrito más abajo, siga este [link](#cleaning) para revisarlo.

La data preparada para ser analizada podemos encontrarla en el archivo IDE_limpio.xlsx, descarguelo usando el link y guárdelo en la carpeta _data_. Desde ahí comience:

```{r, eval=TRUE}
library(openxlsx)
folder='data'
fileName='IDE_limpio.xlsx'
fileToRead=file.path(folder,fileName)
provinciasNew=read.xlsx(fileToRead)
```

La idea es super simple, queremos, en este caso, saber qué provincias son lo sufientemente **similares entre sí** para decidir que formen un grupo homogeneo o conglomerado. Lo atractivo, en este caso, es que las múultiples variables que se tienen permiten darle un **perfil** particular a cada provincia, lo que permite _acercarlas_ o _alejarlas_ basandonos justamente en tal similitud.
podemos usar muchas características de las provincias (esta es una técnica multivariada) para agruparlas. 

Veremos aquí una técnica de conglomeración conocidad como la **jerárquica**, limitaremos la aplicación a que multiples variables numéricas.

Veamos que data tenemos, y de qué tipo son:
```{r, eval=TRUE}
str(provinciasNew)
```

Las variables que nos interesan son:

* Población con acta nacimiento o DNI, es un **porcentaje**.
* Médicos por cada 10,000 habitantes, es una **razón**.
* Tasa asistencia a Secundaria (Pob. 12 a 16 años), es un **porcentaje**.
* Viviendas con agua y desague, es un **porcentaje**.		
* Viviendas electrificadas, es un **porcentaje**.

Hagamos el subconjunto de variables que necesitemos (de la columna 7 a la 11):
```{r, eval=TRUE}
prov_sub=provinciasNew[c(7:11)]
# vemos:
head(prov_sub)
```

Estas variables son las que necesitan la función para llevar a cabo la técnica, pero hay que hacer algunos ajustes.

1) El subconjunto de la data no tiene como identificar a que caso corresponde cada fila. Aquí podemos usar el _ubigeo_ o el _nombre_ tal que reemplace los números que figuran a la izquierda de la data. El único requisito es que estos valores sean **únicos**. El ubigeo lo es, pero debemos verificar si el nombre de la provincia lo es:

```{r, eval=TRUE}
numFilas=nrow(prov_sub)
sinRepetir=length(unique(provinciasNew$provinciaNombre))
numFilas==sinRepetir # si es TRUE, no hay repetidos
```

Como no hay nombres repetidos:
```{r, eval=TRUE}
row.names(prov_sub)=provinciasNew$provinciaNombre
# ver
head(prov_sub)
```

Ya no están los numeros; y ya tenemos asegurado que los nombres se mantengan durante el análisis.

2. Las variables vienen con unidades de medición diferentes. Para que la técnica de comglomerados funcione adecuadamente, debemos llevar las variables a valores independientes de la unidad de medida:

```{r, eval=TRUE}
prov_sub.scaled<- scale(prov_sub)
# resultado
head(prov_sub.scaled)
```

Aquí tenemos puntuaciones Z, para todas la columnas.

3. El paso que sigue es clave. Aquí debemos generar información sobre las similitudes o 'distancias' entre las filas basándonos en las columnas:

```{r, eval=TRUE}
prov_sub_d <- dist(prov_sub.scaled)
```


4. En _prov_sub_d_ están las distancias; lo cual utilizamo en la función aglomeradora.
```{r, eval=TRUE}
prov_sub_clusters=hclust(prov_sub_d)
```

Los resultados se exploran visualmente mediante el **dendograma**:
```{r, eval=TRUE}
plot(prov_sub_clusters,main='Conglomerados',cex=0.4)
```

Como se ve arriba, cada provincia comenzó aislada en la parte inferior, y luego se van conglomerando. 

La técnica muestra el proceso de conglomeración. Por lo general, uno revisa el dendograma y  decide a partir de él  cuantos conglomerados usar, a partir de esta gráfica. 

Si uno decidiése por cuatro conglomerados, tendría que calcularse así:

```{r, eval=TRUE}
grupo <- cutree(prov_sub_clusters, k = 4)
```

Para ver quiénes están en el conglomerado, por ejemplo, '4', sería así:
```{r, eval=TRUE}
row.names(prov_sub)[grupo == 4] 
```

Este es un data frame con los grupos asignados a cada provincia:

```{r, eval=TRUE}
asignaciones=as.data.frame(grupo) # convierto a data frame
# que tengo:
head(asignaciones)
```
```{r, eval=TRUE}
asignaciones$provincia=row.names(asignaciones) # paso los nombres a columna 
# que tengo:
head(asignaciones)
```

Reseteo los nombres de fila:
```{r, eval=TRUE}
row.names(asignaciones)=NULL
# esto queda así
head(asignaciones)
```

**IMPORTANTE**: Si calculásemos clusters con data de otro año, podríamos comparar si una provincia ha mejorado o decaído de uno a otro a año.

Nótese que el número asignada a cada conglomerado no representa ningun orden; es decir, estar en el cluster 2 puede ser mejor que estar en el 1. Para evitar ello, debemos juntar la data qué informa sobre el cluster y la data original:

```{r, eval=TRUE}
# recuperando los datos de provincia en una copia.
# asi evito alterar prov_sub en caso la necesite intacta luego.
copia_prov_sub=prov_sub
copia_prov_sub$provincia=row.names(copia_prov_sub)
```

Juntemos la data:
```{r, eval=TRUE}
# Añadiendo la información de clusters a esta data:
copia_prov_sub=merge(copia_prov_sub,asignaciones)
row.names(copia_prov_sub)=NULL # reseteando nombre de fila
str(copia_prov_sub)
```

Sabiendo que grupo está en la data, pedimos los valores medios:
```{r, eval=TRUE}

aggregate(cbind(identificacion2012,medicos2012,escolaridad2012,AguaDesague2012,
                electrificacion2012) ~ grupo, data=copia_prov_sub,FUN=mean)
```

Como se nota, usando la media podríamos renumerar los clustes con valores más informativos, es decir, recodificar esa columna para que represente orden ascendente o descendente (y no en desorden cómo está ahi: peor es el 2, de ahi 3, de ahi 1 y el mejor el 4). 

El corte en 4 grupos se puede ver así:

```{r, eval=TRUE}
plot(prov_sub_clusters, cex = 0.6)
rect.hclust(prov_sub_clusters, k = 4, border = c('red','blue','gray','green'))
```


Resaltar el 4 sería así:

```{r, eval=TRUE}
plot(prov_sub_clusters, cex = 0.6)
rect.hclust(prov_sub_clusters, k = 4, border = 'red',which = 4)
```

El dendograma nos muestra el proceso de aglomeración, pero nosotros debemos decidir con cuantos clusters quedarnos.

Lo interesante es que no hay una manera certera de decidir cuántos clusters debemos elegir. De hecho hay diversos indices creados para _sugerir_ cuántos. Estos indices podemos verlos de esta manera:

```{r, eval=TRUE}
library(NbClust)

#usar aqui las variables normalizadas: prov_sub.scaled
nb <- NbClust(prov_sub.scaled, method = "complete") # 'dist' usa este método por defecto

```

Del reporte anterior, se ve que la sugerencia depende de la regla de la mayoria: usar TRES clusters. El resultado anterior se puede plotear así:

```{r, eval=TRUE}
library(factoextra) # toma su tiempo
fviz_nbclust(nb) + theme_minimal()
```

El paquete _factoextra_ tiene su propia función para calcular conglomerados ('eclust').

```{r, eval=TRUE}
# resultado de factoExtra (FE)
prov_sub_clusters_FE <- eclust(prov_sub.scaled, FUNcluster ="hclust", k = 4,
                method = "complete", graph = FALSE) 
fviz_dend(prov_sub_clusters_FE, rect = TRUE, show_labels = FALSE) 
```

Este resultado también guarda los clusters:
```{r, eval=TRUE}
head(as.data.frame(prov_sub_clusters_FE$cluster))
```

Nótese que los resultados de esta función han salido iguales a los hallados con el **hclust** básico.

Lo importante de este paquete es que nos permite validar qué tan buena ha sido nuestra aglomeración. Para ello calcula las **siluetas**:

```{r, eval=TRUE}
fviz_silhouette(prov_sub_clusters_FE)
```

Un valor cercano a **uno** indica que el valor está bien aglomerado, si está cercano a cero indica que esta entre dos conglomerados, y si es negativo, que está en un cluster erróneo (al algoritmo acabó y no pudo ubicarlo en el grupo adecuado).

El objeto *prov_sub_clusters_FE* guarda esta información de silueta. Para acceder a ella tenemos:

```{r, eval=TRUE}
siluetas <-prov_sub_clusters_FE$silinfo$widths
```

Para ver los que están mal agrupados tenemos:

```{r, eval=TRUE}
siluetas[siluetas$sil_width<0,]
```

Ya sabemos que si a loss resultados que acabamos de ver les aplicamos la función *as.data.frame* podremos manipularlos como cualquier tabla de datos.

### NOTA IMPORTANTE:

Cuando utilizamos la función **scale** para normalizar las variables usamos los valores por defecto de esa función, es decir, la _media_ y la _desviación típica_. Podríamos usar las medianas y la desviaciónes de la mediana en vez de ellas:

```{r, eval=TRUE}
medians = apply(prov_sub,2,median)
mads = apply(prov_sub,2,mad)
prov_sub.scaled2 = scale(prov_sub,center=medians,scale=mads)
```

Habrán cambios cualitativos si usamos estos nuevos valores? 

```{r, eval=FALSE, echo=FALSE}
library(NbClust)
nb <- NbClust(df.scaled2, distance = "minkowski", min.nc = 2,
        max.nc = 10, method = "ward.D2", index ="all")

```
```{r, eval=FALSE, echo=FALSE}
library(factoextra)
fviz_nbclust(nb) + theme_minimal()
```


```{r, eval=FALSE, echo=FALSE}
res.hc <- eclust(df.scaled2, "hclust", k = 3,hc_metric='minkowski',
                method = "ward.D2", graph = FALSE) 
fviz_dend(res.hc, rect = TRUE, show_labels = FALSE) 
```
```{r, eval=FALSE, echo=FALSE}
fviz_silhouette(res.hc)
```



```{r, eval=FALSE, echo=FALSE}
sil <-res.hc$silinfo$widths
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]
```


<a id='cleaning'></a>

## Preparación de la data multivariada

La data original es esta en el archivo "idePeru.xlsx" que lo puede descargar de [aquí](https://github.com/PoliticayGobiernoPUCP/EstadisticaPoliticaGobiernoII/raw/master/sesiones/data/idhPeru.xlsx).

Descargue la data y guardela en la carpeta _data_. Desde ahí comience a trabajar:

```{r, eval=TRUE}
library(openxlsx)
folder='data'
fileName='idePeru.xlsx'
fileToRead=file.path(folder,fileName)
cualHoja=1
datos<- read.xlsx(fileToRead, cualHoja)
head(datos,10)
```

**Openxlsx** es muy eficiente, pues inclusive omite columnas vacías. Sin embargo, vemos que la data no inicia donde debe, los titulos no corresponden a las columnas; y, aunque que las Provincias figuran en cada fila, los Departamentos no están en las filas, sino como titulos para las provincias (y encima no hay distritos, aunque la hoja muestra ese nombre). Arreglemos esto.


```{r, eval=TRUE}
filaInicial=4
datos=read.xlsx(fileToRead, 
                sheet = 1, 
                startRow = filaInicial, 
                skipEmptyRows = TRUE, skipEmptyCols = TRUE)
```

Los reportes de este tipo sueles añadir información innecesaria al final, verifiquemos:

```{r, eval=TRUE}
tail(datos,10)
```

La data acaba en la fila 222, con la provincia de Purus. Eliminemos filas innecesarias:

```{r, eval=TRUE}
datos=datos[-c(223:226),]
```

El encabezado aun tiene problemas:
```{r, eval=TRUE}
head(datos)
```

Las filas 1 al 3 nos innecesarias:

```{r, eval=TRUE}
datos=datos[-c(1:3),]
```

Separemos y guardemos los nombres de regiones y sus codigos (ubigeos) :
```{r, eval=TRUE}
queColumnas=c(1,2)
regiones=datos[,queColumnas]
head(regiones,10)
```
Podemos quedarnos con los casos completos por fila:
```{r, eval=TRUE}
regiones=regiones[complete.cases(regiones),]
head(regiones,10)
```

Cambiemos nombres:
```{r, eval=TRUE}
nombresNuevos=c('regionUbigeo','regionNombre')
names(regiones)=nombresNuevos
head(regiones)
```

De igual manera, guardemos la info de las provincias:
```{r, eval=TRUE}
queColumnas=c(2)
provincias=datos[,-queColumnas]
head(provincias,10)
```

Veamos que columnas tenemos:
```{r, eval=TRUE}
names(provincias)
```

X1 y X3 son necesarias. Eliminemos todas las otras que comiencen con X (los rankings):

```{r, eval=TRUE}
dejandoUno=seq(4,16,2) # 4,6,8, etc.
queColumnas=c(dejandoUno)
provincias=provincias[,-queColumnas]
head(provincias,10)
```

Nuevamente, podemos quedarnos con los casos completos por fila:
```{r, eval=TRUE}
provincias=provincias[complete.cases(provincias),]
head(provincias,10)
```

Renombremos las dos primeras columnas
```{r, eval=TRUE}
names(provincias)[c(1,2)]=c('provinciaUbigeo','provinciaNombre')
head(provincias)
```

Con la misma lógica, simplifiquemos los otros nombres de columnas:
```{r, eval=TRUE}
names(provincias)
```

```{r, eval=TRUE}
names(provincias)[c(3:9)]=c('pob2012','ide2012','identificacion2012','medicos2012','escolaridad2012','AguaDesague2012','electrificacion2012')
head(provincias)
```

Podriamos calcular la media de la población:
```{r, eval=TRUE}
mean(provincias$pob2012)
```

Si aplicas una función numérica, y sale error, es por que quizas los datos tengan problemas:
```{r, eval=TRUE}
str(provincias)
```

Las funciones numéricas no pueden aplicarse a los textos ('chr'), por ende:
```{r, eval=TRUE}
provincias[,c(3:9)]=lapply(provincias[,c(3:9)],as.numeric)
str(provincias)
```
Ahora si funciona:
```{r, eval=TRUE}
mean(provincias$pob2012)
```

Podemos ahora crear la columna _regionUbigeo_, usando _provinciaUbigeo_; sólo hay que quedarnos con los dos (2) primeros digitos de esa columna (nótese que están como texto).
```{r, eval=TRUE}
#primero la duplicamos pero con otro nombre (por defecto nueva columna va al final)
provincias$regionUbigeo=provincias$provinciaUbigeo
```

A la nueva columna le reemplazamos con '0000' todo valor luego de los primeros dos digitos:
```{r, eval=TRUE}
substr(provincias$regionUbigeo,3,6)='0000'
```

Podemos reubicar la posición de la última columna:
```{r, eval=TRUE}
# jugando con la posiciones:
provincias=provincias[,c(10,1:9)] 
```

Tenemos entonces:
```{r, eval=TRUE}
head(provincias,10)
```

Hagamos el '_merge_', entre los datos de **regiones** y **provincias**:
```{r, eval=TRUE}
provinciasNew=merge(provincias,regiones,by.x = 'regionUbigeo',
                    by.y='regionUbigeo',
                    all.x = TRUE) # Esto detecta si alguna provincia no encontró Region.
```

Si el merge salió bien, provincias tendrá una nueva columna (al final) con el nombre de la Región:
```{r, eval=TRUE}
names(provinciasNew)
```


Movamos de posición esa columna nueva:
```{r, eval=TRUE}
provinciasNew=provinciasNew[,c(1:2,11,3:10)]
```

Asi quedó:

```{r, eval=TRUE}
head(provinciasNew)
```



```{r, echo=FALSE, eval=FALSE}
write.xlsx(provinciasNew,"data/IDE_limpio.xlsx")
```

[ir al inicio de la página](#beginning)

####[VOLVER AL SILABO](https://politicaygobiernopucp.github.io/EstadisticaPoliticaGobiernoII/)
