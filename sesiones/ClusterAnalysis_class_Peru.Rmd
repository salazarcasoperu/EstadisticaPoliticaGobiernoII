<br> 
<center><img src="http://i.imgur.com/tveTlt8.png" width="300"></center>

## Course: Estadística Para el Análisis Político II<br> Semestre 2017-II<br> 
### Prof. José Manuel Magallanes, PhD 
____
## **Análisis Multivariado**
## Técnicas Descriptivas y Exploratorias (Parte I)
____

<a id='beginning'></a>

Análizar conglomerados es un proceso complejo. Sobre todo pues los datos deben estar bien entendidos y el propósito del estudio bastante claro para saber qué camino tomar.

Nuestro interés está, en este caso, en los datos del Indice de Densidad del Estado que tuvo a bien crear el PNUD. Estos datos están en Excel, lo cual no sería mayor problema, sino fuera por qué las hojas tienen títulos innecsarios para nuestro análisis. Veamos la data de la primera hoja, que corresponde a este índice para el 2012:

```{r, eval=FALSE}
library(openxlsx)
folder='data'
fileName='idePeru.xlsx'
fileToRead=file.path(folder,fileName)
cualHoja=1
datos<- read.xlsx(fileToRead, cualHoja)
head(datos,10)
```

**Openxlsx** es muy eficiente, pues inclusive omite columnas vacías. Sin embargo, vemos que la data no inicia donde debe, los titulos no corresponden a las columnas; y, aunque que las Provincias figuran en cada fila, los Departamentos no están en las filas, sino como titulos para las provincias (y encima no hay distritos, aunque la hoja muestra ese nombre). Arreglemos esto.


```{r, eval=FALSE}
filaInicial=4
datos=read.xlsx(fileToRead, 
                sheet = 1, 
                startRow = filaInicial, 
                skipEmptyRows = TRUE, skipEmptyCols = TRUE)
```

Los reportes de este tipo sueles añadir información innecesaria al final, verifiquemos:

```{r, eval=FALSE}
tail(datos,10)
```

La data acaba en la fila 222, con la provincia de Purus. Eliminemos filas innecesarias:

```{r, eval=FALSE}
datos=datos[-c(223:226),]
```

El encabezado aun tiene problemas:
```{r, eval=FALSE}
head(datos)
```

Las filas 1 al 3 nos innecesarias:

```{r, eval=FALSE}
datos=datos[-c(1:3),]
```

Separemos y guardemos los nombres de regiones y sus codigos (ubigeos) :
```{r, eval=FALSE}
queColumnas=c(1,2)
regiones=datos[,queColumnas]
head(regiones,10)
```
Podemos quedarnos con los casos completos por fila:
```{r, eval=FALSE}
regiones=regiones[complete.cases(regiones),]
head(regiones,10)
```

Cambiemos nombres:
```{r, eval=FALSE}
nombresNuevos=c('regionUbigeo','regionNombre')
names(regiones)=nombresNuevos
head(regiones)
```

De igual manera, guardemos la info de las provincias:
```{r, eval=FALSE}
queColumnas=c(2)
provincias=datos[,-queColumnas]
head(provincias,10)
```

Veamos que columnas tenemos:
```{r, eval=FALSE}
names(provincias)
```

X1 y X3 son necesarias. Eliminemos todas las otras que comiencen con X (los rankings):

```{r, eval=FALSE}
dejandoUno=seq(4,16,2) # 4,6,8, etc.
queColumnas=c(dejandoUno)
provincias=provincias[,-queColumnas]
head(provincias,10)
```

Nuevamente, podemos quedarnos con los casos completos por fila:
```{r, eval=FALSE}
provincias=provincias[complete.cases(provincias),]
head(provincias,10)
```

Renombremos las dos primeras columnas
```{r, eval=FALSE}
names(provincias)[c(1,2)]=c('provinciaUbigeo','provinciaNombre')
head(provincias)
```

Con la misma lógica, simplifiquemos los otros nombres de columnas:
```{r, eval=FALSE}
names(provincias)
```

```{r, eval=FALSE}
names(provincias)[c(3:9)]=c('pob2012','ide2012','identificacion2012','medicos2012','escolaridad2012','AguaDesague2012','electrificacion2012')
head(provincias)
```

Podriamos calcular la media de la población:
```{r, eval=FALSE}
mean(provincias$pob2012)
```

Si aplicas una función numérica, y sale error, es por que quizas los datos tengan problemas:
```{r, eval=FALSE}
str(provincias)
```

Las funciones numéricas no pueden aplicarse a los textos ('chr'), por ende:
```{r, eval=FALSE}
provincias[,c(3:9)]=lapply(provincias[,c(3:9)],as.numeric)
str(provincias)
```
Ahora si funciona:
```{r, eval=FALSE}
mean(provincias$pob2012)
```

Podemos ahora crear la columna _regionUbigeo_, usando _provinciaUbigeo_; sólo hay que quedarnos con los dos (2) primeros digitos de esa columna (nótese que están como texto).
```{r, eval=FALSE}
#primero la duplicamos pero con otro nombre (por defecto nueva columna va al final)
provincias$regionUbigeo=provincias$provinciaUbigeo
```

A la nueva columna le reemplazamos con '0000' todo valor luego de los primeros dos digitos:
```{r, eval=FALSE}
substr(provincias$regionUbigeo,3,6)='0000'
```

Podemos reubicar la posición de la última columna:
```{r, eval=FALSE}
# jugando con la posiciones:
provincias=provincias[,c(10,1:9)] 
```

Tenemos entonces:
```{r, eval=FALSE}
head(provincias,10)
```

Hagamos el '_merge_', entre los datos de **regiones** y **provincias**:
```{r, eval=FALSE}
provinciasNew=merge(provincias,regiones,by.x = 'regionUbigeo',
                    by.y='regionUbigeo',
                    all.x = TRUE) # Esto detecta si alguna provincia no encontró Region.
```

Si el merge salió bien, provincias tendrá una nueva columna (al final) con el nombre de la Región:
```{r, eval=FALSE}
names(provinciasNew)
```


Movamos de posición esa columna nueva:
```{r, eval=FALSE}
provinciasNew=provinciasNew[,c(1:2,11,3:10)]
```

Asi quedó:

```{r, eval=FALSE}
head(provinciasNew)
```

Muy bien. Ahora comencemos a pensar en los conglomerados.

La idea es super simple, queremos, en este caso, saber que provincias son lo sufientemente similares entre sí para decir que forman un grupo homogeneo o conglomerado. Lo atractivo, en este caso, es que podemos usar muchas características de las provincias (esta es una técnica multivariada) para agruparlas. Una limitación a tener en cuenta, es que la técnica de conglomerados que veremos está limitada a variables numéricas.

Antes que nada debemos cambiar el nombre de las filas:
```{r, eval=FALSE}
head(provinciasNew)
```

Arriba, hay numeros en cada fila, ahora deben estar los nombres de las provincias, si es que no comparten el mismo nombre:
```{r, eval=FALSE}
length(provinciasNew$provinciaNombre)==length(unique(provinciasNew$provinciaNombre))
```

Como no hay nombres repetidos:
```{r, eval=FALSE}
row.names(provinciasNew)=provinciasNew$provinciaNombre
head(provinciasNew)
```

Ya no están los numeros!



Ahora exploremos las variables:

```{r, eval=FALSE}
summary(provinciasNew)
```

La función _summary_ de R no ayuda mucho para comparar los valores. Podemos crear nuestra propia función para explorar los datos:
```{r, eval=FALSE}
df=provinciasNew[c(7:11)]
descriptivos <- data.frame(
  Min = apply(df, 2, min), # minimum
  Med = apply(df, 2, median), # median
  Mean = apply(df, 2, mean), # mean
  SD = apply(df, 2, sd), # Standard deviation
  Max = apply(df, 2, max) # Maximum
  )
descriptivos <- round(descriptivos, 1)
head(descriptivos)
```

Estas son todas las variables numéricas (obviando población y el índice en sí), y nos damos cuenta que:

* Población con acta nacimiento o DNI, es un **porcentaje**.
* Médicos por cada 10,000 habitantes, es una **razón**.
* Tasa asistencia a Secundaria (Pob. 12 a 16 años), es un **porcentaje**.
* Viviendas con agua y desague, es un **porcentaje**.		
* Viviendas electrificadas, es un **porcentaje**.

Por lo general, las características de las unidades de estudio (aqui Provincias) vienen con unidades de medición diferentes. Para que la técnica de comglomerados funcione adecuadamente, debemos llevar las variables a valores independientes de la unidad de medida:
```{r, eval=FALSE}
df.scaled1 <- scale(df)
head(df.scaled1)
```

Una vez que los datos se han reescalado, debemos generar información sobre las similitudes o 'distancias' entre las filas basándonos en las columnas:

```{r, eval=FALSE}
d1 <- dist(df.scaled1)
```


En _d1_ están las distancias, esto permite usar el algoritmo de conglomeración jerárquica.
```{r, eval=FALSE}
d1.clusters=hclust(d1)
```

Los resultados se exploran visualmente mediante el **dendograma**:
```{r, eval=FALSE}
plot(d1.clusters,main='Conglomerados',cex=0.4)
```
Como se ve arriba, cada provincia comenzó aislada, y luego se van conglomerando. Uno debe decidir cuantos conglomerados usar, a partir de esta gráfica. Escojamos cuatro conglomerados:
```{r, eval=FALSE}
grupo <- cutree(d1.clusters, k = 4)
table(grupo)
```

Para ver uno de los conglomerados:
```{r, eval=FALSE}
row.names(df)[grupo == 4] 
```

O de manera gráfica:
```{r, eval=FALSE}
plot(d1.clusters, cex = 0.6)
rect.hclust(d1.clusters, k = 4, border = c('red','blue','gray','green'))
```
Nótese que el número asignada a cada conglomerados no representa ningun orden. Aqui observamos el bloque 4:
```{r, eval=FALSE}
plot(d1.clusters, cex = 0.6)
rect.hclust(d1.clusters, k = 4, border = 'red',which = 4)
```

El dendograma nos muestra el proceso de aglomeración, pero nosotros debemos decidir con cuantos clusters quedarnos.

Lo interesante es que no hay una manera certera de decidir cuántos clusters debemos elegir. De hecho hay diversos indices creados para _sugerir_ cuántos. Estos indices podemos verlos de esta manera:

```{r, eval=FALSE}
library(NbClust)
nb <- NbClust(df.scaled1, method = "complete") # 'dist' usa este método por defecto

```

Del reporte anterior, se ve la sugerencia depende de la regla de la mayoria. El resultado anterior se puede plotear así:

```{r, eval=FALSE}
library(factoextra) # toma su tiempo
fviz_nbclust(nb) + theme_minimal()
```

El paquete _factoextra_ tiene su propia función para calcular conglomerados ('eclust').

```{r, eval=FALSE}
res.hc <- eclust(df.scaled1, FUNcluster ="hclust", k = 4,
                method = "complete", graph = FALSE) 
fviz_dend(res.hc, rect = TRUE, show_labels = FALSE) 
```

Este paquete permite validar qué tan buena ha sido nuestra aglomeración:

```{r, eval=FALSE}
fviz_silhouette(res.hc)
```

Un valor cercano a **uno** indica que el valor está bien aglomerado, si está cercano a cero indica que esta entre dos conglomerados, y si es negativo, que está en un cluster erróneo (al algoritmo acabó y no pudo ubicarlo en el grupo adecuado).

El objeto _res.hc_ guarda esta información de silueta. Para acceder a ella tenemos:

```{r, eval=FALSE}
siluetas <-res.hc$silinfo$widths
```

Para ver los que están mal agrupados tenemos:

```{r, eval=FALSE}
siluetas[siluetas$sil_width<0,]
```

Cuando utilizamos la función **scale** para normalizar las variables usamos los valores por defecto de esa función, es decir, la media y la desviación típica. Podríamos usar las medianas y la desviaciónes de la mediana en vez de ellas:

```{r, eval=FALSE}
medians = apply(df,2,median)
mads = apply(df,2,mad)
df.scaled2 = scale(df,center=medians,scale=mads)
```

Habrán cambios cualitativos si usamos estos nuevos valores? 

```{r, eval=FALSE, echo=FALSE}
library(NbClust)
nb <- NbClust(df.scaled2, distance = "minkowski", min.nc = 2,
        max.nc = 10, method = "ward.D2", index ="all")

```
```{r, eval=FALSE, echo=FALSE}
library(factoextra)
fviz_nbclust(nb) + theme_minimal()
```


```{r, eval=FALSE, echo=FALSE}
res.hc <- eclust(df.scaled2, "hclust", k = 3,hc_metric='minkowski',
                method = "ward.D2", graph = FALSE) 
fviz_dend(res.hc, rect = TRUE, show_labels = FALSE) 
```
```{r, eval=FALSE, echo=FALSE}
fviz_silhouette(res.hc)
```



```{r, eval=FALSE, echo=FALSE}
sil <-res.hc$silinfo$widths
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]
```


